# VAE
- [ ] Auto-Encoding Variational Bayes (Kingma & Welling, 2013/2014)
- [ ] Stochastic Backpropagation and Approximate Inference in Deep Generative Models (Rezende et al., 2014)
- [ ] Importance Weighted Autoencoders (IWAE) (Burda et al., 2015)
- [ ] Deep Latent Gaussian Models (Rezende & Mohamed, 2015)
- [ ] Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al., 2017)
- [ ] Disentangling by Factorising (Kim & Mnih, 2018)
- [ ] InfoVAE: Information Maximizing Variational Autoencoders (Zhao et al., 2017)
- [ ] Adversarial Autoencoders (Makhzani et al., 2015)
- [ ] Variational Inference with Normalizing Flows (Kingma et al., 2016)
- [ ] PixelVAE: A Latent Variable Model for Natural Images (Gulrajani et al., 2016)
- [ ] Discrete Variational Autoencoders (Rolfe, 2016)
- [ ] NVAE: A Deep Hierarchical VAE (Vahdat & Kautz, 2020)
- [ ] Denoising Variational Autoencoders (Im et al., 2015)

# Autoregressive Models
- [ ] PixelRNN and PixelCNN: Generating Images with Autoregressive Models (van den Oord et al., 2016)
- [ ] WaveNet: A Generative Model for Raw Audio (van den Oord et al., 2016)
- [ ] Neural Autoregressive Distribution Estimator (NADE) (Larochelle & Murray, 2011)
- [ ] PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications (Salimans et al., 2017)
- [ ] Autoregressive Image Models with Auxiliary Decoders (Chen et al., 2017)
- [ ] Generating Diverse High-Fidelity Images with VQ-VAE-2 (Razavi et al., 2019)
- [ ] Parallel WaveNet: Fast High-Fidelity Speech Synthesis (van den Oord et al., 2017)
- [ ] SampleRNN: An Unconditional End-to-End Neural Audio Generation Model (Mehri et al., 2016)
- [ ] Video Pixel Networks (Kalchbrenner et al., 2017)
- [ ] MaskGAN: Better Text Generation via Filling in the ____ (Fedus et al., 2018)


# Flow Models
- [ ] Planar Flows
- [ ] NICE: Non-linear Independent Components Estimation
- [ ] Real NVP: Density Estimation Using Real NVP
- [ ] FFJORD
- [ ] MAF: Masked Autoregressive Flows
- [ ] IAF: Inverse Autoregressive Flows
- [ ] Parallel Wavenet
- [ ] MintNet
- [ ] Gaussianization Flows
- [ ] Glow: Generative Flow with Invertible 1x1 Convolutions
- [ ] FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models
- [ ] Neural ODEs: Neural Ordinary Differential Equations
- [ ] Residual Flows for Invertible Generative Modeling
- [ ] i-ResNet: Invertible Residual Networks
- [ ] Improving VAEs with Flow
- [ ] Autoregressive Flows
- [ ] Continuous Normalizing Flows

# GANS
- [ ] GANs
- [ ] DCGAN
- [ ] WGAN
- [ ] BiGAN
- [ ] Improved Training of Wasserstein GANs
- [ ] Conditional GANs (cGAN)
- [ ] WGAN-GP
- [ ] pix2pix
- [ ] CycleGAN	
- [ ] BigGAN: Large Scale GAN Training
- [ ] ProgressiveGAN	
- [ ] StyleGAN
- [ ] StyleGAN2
- [ ] Inception Score	
- [ ] FID
- [ ] Large Scale GAN Training for High Fidelity Natural Image Synthesis
- [ ] Self-Attention Generative Adversarial Networks

# EBM
- [ ] A Learning Algorithm for Boltzmann Machines (Ackley, Hinton, & Sejnowski, 1985)
- [ ] Learning Representations by Back-Propagating Errors (Hinton, 1986)
- [ ] Contrastive Divergence Learning (Hinton, 2002)
- [ ] Energy-Based Generative Neural Networks (LeCun et al., 2006)
- [ ] Learning with Energy-Based Models (LeCun, Chopra, Hadsell, 2006)
- [ ] Energy-Based Models (Original EBM Paper)
- [ ] Contrastive Divergence Learning
- [ ] Boltzmann Machines: Learning and Inference
- [ ] Score Matching for Energy-Based Models
- [ ] Noise-Contrastive Estimation
- [ ] Flow Contrastive Estimation of Energy-Based Models
- [ ] Learning Deep Energy Models
- [ ] Deep Energy-Based Models for Text Generation
- [ ] Cooperative Training of Energy-Based Models and Latent Variable Models
- [ ] JEM: Joint Energy-Based Model Training
- [ ] Energy-Based Models for Continual Learning
- [ ] Energy-Based Models in NLP

# Diffusion Models
- [ ] Deep Unsupervised Learning using Nonequilibrium Thermodynamics (Sohl-Dickstein et al., 2015)
- [ ] Denoising Diffusion Probabilistic Models (DDPM)
- [ ] Improved Denoising Diffusion Probabilistic Models
- [ ] Score-Based Generative Modeling through Stochastic Differential Equations
- [ ] Diffusion Models Beat GANs on Image Synthesis
- [ ] Cascaded Diffusion Models for High-Resolution Image Synthesis (Ho et al., 2021)
- [ ] Latent Diffusion Models (LDMs)
- [ ] Denoising Diffusion Implicit Models (DDIM)
- [ ] Conditional Image Synthesis with Score-Based Diffusion Models
- [ ] Elucidating the Design Space of Diffusion-Based Generative Models
- [ ] Guided Diffusion
- [ ] Stable Diffusion
- [ ] Transfer Learning for Diffusion Models
- [ ] Latent Schr√∂dinger Bridge Diffusion Model for Generative Learning
- [ ] Elucidating the Design Space of Diffusion-Based Generative Models (Karras et al., 2022)
- [ ] Score Matching with Langevin Dynamics (Song & Ermon, 2019)
- [ ] Video Diffusion Models (Ho et al., 2022)
- [ ] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (Nichol et al., 2021)
- [ ] DreamFusion: Text-to-3D Using 2D Diffusion (Poole et al., 2022)
- [ ] Consistent Diffusion Models (Song et al., 2023)
- [ ] Score-Based Generative Models for Compositional Generation (Jing et al., 2022)





